{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267b648f-f2e1-47a7-ba2f-7a01bf964221",
   "metadata": {},
   "source": [
    "# Deploying WhisperX on Amazon SageMaker Utilizing LMIv16\n",
    "In this example we take a look at deploying WhisperX on Amazon SageMaker Real-Time Endpoints using the LMI v16 container. For models that aren't natively supported by a backend such as vLLM yet, you can use the Python engine via the container to load the model onto a GPU instance like we do in this case. \n",
    "\n",
    "At the moment of this notebook, WhisperX does not have native vLLM support (that I'm aware of), but whisper large v3 does: https://docs.vllm.ai/en/v0.7.0/getting_started/examples/whisper.html. That you can use natively via a vLLM backend.\n",
    "\n",
    "### Other Hosting Options/Routes\n",
    "In this example we serialize the audio file into a numpy array using WhisperX's library. You can alternatively pass in an S3 URI as the payload and do this serialization within the container. Another option for more hybrid inference workloads is Async Inference: https://dev.to/makawtharani/deploying-whisperx-on-aws-sagemaker-as-asynchronous-endpoint-17g6. Here you can enable scale-down to zero and have managed queuing as part of the solution.\n",
    "\n",
    "### Docker Testing\n",
    "We always recommend testing with Docker on an EC2/EKS node of the instance you want to deploy on, this allows for quick debugging of your model.py and serving configuration. Here's a sample command:\n",
    "\n",
    "```\n",
    "#Pull image\n",
    "docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128\n",
    "\n",
    "#Start container, adjust for path of artifacts\n",
    "docker run \\\n",
    "  --gpus all \\\n",
    "  -v /home/ubuntu:/opt/ml/model \\\n",
    "  -p 8080:8080 \\\n",
    "  763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128 \\\n",
    "  serve\n",
    "\n",
    "```\n",
    "\n",
    "### Additional Resources\n",
    "- Docker Debug Tutorial: https://www.youtube.com/watch?v=UQHufr-DToE\n",
    "- Large Model Inference Container Intro: https://www.youtube.com/watch?v=Q-Kz5Yi0QiQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b18e328-8cce-4b74-8487-8c2bb0d2a479",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Also install WhisperX if trying to invoke the endpoint, there might be some dependency clashes if using SM Notebook Instances. We ran this notebook in a ml.g5.4xlarge to test with Docker on a SM Classic Notebook Instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62432165-e10b-4e88-aeb6-c1d108f61548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker import image_uris\n",
    "import subprocess\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "instance_type = \"ml.g5.4xlarge\"\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name\n",
    "bucket = session.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "CONTAINER_VERSION = \"0.34.0-lmi16.0.0-cu128\"\n",
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:{CONTAINER_VERSION}\"\n",
    "print(f\"Using image URI: {inference_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b9854-a4dd-496d-8a5a-2509d6152ff5",
   "metadata": {},
   "source": [
    "## Prepare Model Artifacts\n",
    "For LMI container we prepare a\n",
    "- model.py: Custom inference loading and pre/post processing code\n",
    "- requirements.txt: Dependencies to install, whisperx in this case\n",
    "- serving.properties: Specify engine, Python in this case. For vLLM supported models use that backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3750b-2792-4059-94b2-7b5120e8bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "from djl_python import Input\n",
    "from djl_python import Output\n",
    "import torch\n",
    "import whisperx\n",
    "import gc\n",
    "from whisperx.diarize import DiarizationPipeline\n",
    "import numpy as np\n",
    "\n",
    "# Set HF Token\n",
    "HF_TOKEN = \"Add your HF Token here\"\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# set instance and compute types\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# suggest GPU instance for WhisperX\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"float32\"\n",
    "\n",
    "class WhisperXModel(object):\n",
    "    \"\"\"\n",
    "    Deploying WhisperX with DJL Serving\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, properties: dict):\n",
    "        \"\"\"\n",
    "        Initialize model.\n",
    "        \"\"\"\n",
    "        logging.info(os.listdir())\n",
    "        logging.info(\"-----------------\")\n",
    "        logging.info(properties)\n",
    "\n",
    "        self.model = whisperx.load_model(\"small\", device, compute_type=compute_type)\n",
    "        self.model_a, self.metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
    "        self.diarization_model = DiarizationPipeline(use_auth_token=os.getenv(\"HUGGINGFACE_TOKEN\"), device=device)\n",
    "        self.initialized = True\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Custom service entry point function.\n",
    "\n",
    "        :param inputs: the Input object holds the text for the WhisperX model to infer upon\n",
    "        :return: the Output object to be send back\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Sample input: {\"audio_array\": numpy serialization of audio file}\n",
    "            data = inputs.get_as_json()\n",
    "            logging.info(\"-----------------\")\n",
    "            logging.info(data)\n",
    "            logging.info(type(data))\n",
    "            logging.info(\"-----------------\")\n",
    "            \n",
    "            #parse input\n",
    "            audio = data[\"audio_array\"]\n",
    "\n",
    "            # Cast list -> np.ndarray (float32), as required by whisperx\n",
    "            if isinstance(audio, list):\n",
    "                audio = np.asarray(audio, dtype=np.float32)\n",
    "            elif not isinstance(audio, np.ndarray):\n",
    "                return Output().error(\"audio_array must be a list or numpy array\")\n",
    "\n",
    "            # transcription model inference\n",
    "            result = self.model.transcribe(audio)\n",
    "            output = result[\"segments\"]\n",
    "\n",
    "            # alignment model\n",
    "            result = whisperx.align(result[\"segments\"], self.model_a, self.metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "            # diarization\n",
    "            diarize_segments = self.diarization_model(audio)\n",
    "            diarization_result = whisperx.assign_word_speakers(diarize_segments, result)\n",
    "\n",
    "            # parse final output and return as a JSON\n",
    "            final_output = result[\"segments\"] # segments are now assigned speaker IDs\n",
    "            result = {\"outputs\": final_output}\n",
    "            outputs = Output()\n",
    "            outputs.add_as_json(result)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"inference failed\")\n",
    "            # error handling\n",
    "            outputs = Output().error(str(e))\n",
    "        \n",
    "        logging.info(outputs)\n",
    "        logging.info(type(outputs))\n",
    "        logging.info(\"Returning inference---------\")\n",
    "        return outputs\n",
    "\n",
    "\n",
    "_service = WhisperXModel()\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # stateful model\n",
    "        _service.initialize(inputs.get_properties())\n",
    "    \n",
    "    if inputs.is_empty():\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6dcba-05a9-4e75-b999-5ab5758caa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9508a-5d15-4568-8534-b1fb670bf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "whisperx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e145e-232e-44b5-a3ce-b32036ee59ab",
   "metadata": {},
   "source": [
    "## Create SM Constructs\n",
    "- Model: Container & Model Data & Scripts/Serving Properties\n",
    "- EndpointConfig: Instance specs and variants\n",
    "- Endpoint: REST Endpoint to invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7409c-2f9e-4e58-9651-9c9328e9ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tar file with model data + inference code\n",
    "bashCommand = \"tar -cvpzf model.tar.gz model.py requirements.txt serving.properties\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8face79d-d83e-4577-84fe-41373cf34dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload tar.gz to bucket\n",
    "model_artifacts = f\"s3://{bucket}/whisperx/model.tar.gz\"\n",
    "response = s3.meta.client.upload_file('model.tar.gz', bucket, 'whisperx/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513c7ff-280d-4495-ae19-73f073eb87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {model_artifacts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b174ea2-15f6-40f5-8699-3d11acd0e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"djl-whisperx\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image, \"ModelDataUrl\": model_artifacts},\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291286c2-9e7c-400c-baad-edc0ecc225a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = \"djl-whisperx\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "production_variants = [\n",
    "    {\n",
    "        \"VariantName\": \"AllTraffic\",\n",
    "        \"ModelName\": model_name,\n",
    "        \"InitialInstanceCount\": 1,\n",
    "        \"InstanceType\": instance_type,\n",
    "        \"ModelDataDownloadTimeoutInSeconds\": 1800,\n",
    "        \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n",
    "    }\n",
    "]\n",
    "\n",
    "endpoint_config = {\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "    \"ProductionVariants\": production_variants,\n",
    "}\n",
    "\n",
    "endpoint_config_response = client.create_endpoint_config(**endpoint_config)\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86547d-b115-4830-bafd-6a36c87382fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"djl-whisperx\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632c815-c5b7-4fd3-b725-1690d9682fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(45)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06664d64-a9eb-4307-804f-31c9fcec712d",
   "metadata": {},
   "source": [
    "## Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c9d3a-5b66-46cd-a823-27f77efe91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938441bc-4642-4161-b2d3-3dd5953fef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import whisperx\n",
    "import torch\n",
    "\n",
    "# ---- prepare audio ----\n",
    "audio_path = \"test_audio.mp4\" # replace with your Audio file\n",
    "audio = whisperx.load_audio(audio_path)   \n",
    "sample_rate = 16000\n",
    "\n",
    "payload = {\n",
    "    \"audio_array\": audio.tolist(),  # JSON-serializable\n",
    "    \"sample_rate\": sample_rate\n",
    "}\n",
    "\n",
    "# ---- invoke sagemaker endpoint ----\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name = \"us-east-1\")\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,             \n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "# deserialize result\n",
    "result = json.loads(response[\"Body\"].read())\n",
    "print(json.dumps(result, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
